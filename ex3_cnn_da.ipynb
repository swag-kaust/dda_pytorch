{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a36577",
   "metadata": {},
   "source": [
    "# Example 3. CNN + DDA\n",
    "\n",
    "*[Tariq Alkhalifah](https://sites.google.com/a/kaust.edu.sa/tariq/home) and [Oleg Ovcharenko](https://ovcharenkoo.com/), 2021. \"Direct domain adaptation through mutual linear transformations\"*\n",
    "\n",
    "-------\n",
    "This code is modified from [https://github.com/fungtion/DANN_py3](https://github.com/fungtion/DANN_py3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd01a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.fft import rfft2, irfft2, fftshift, ifftshift\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from components.data_loader import GetLoader\n",
    "from components.model import CNNModel\n",
    "from components.test import test\n",
    "import components.shared as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5181f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536c281",
   "metadata": {},
   "source": [
    "### Init paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b942d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to datasets\n",
    "source_dataset_name = 'MNIST'\n",
    "target_dataset_name = 'mnist_m'\n",
    "source_image_root = os.path.join('dataset', source_dataset_name)\n",
    "target_image_root = os.path.join('dataset', target_dataset_name)\n",
    "\n",
    "# Where to save outputs\n",
    "model_root = './out_ex3_cnn_da'\n",
    "os.makedirs(model_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b6ff8",
   "metadata": {},
   "source": [
    "### Init training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ab3ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 222\n"
     ]
    }
   ],
   "source": [
    "cuda = True\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "n_epoch = 100\n",
    "\n",
    "# manual_seed = random.randint(1, 10000)\n",
    "manual_seed = 222\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "print(f'Random seed: {manual_seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120d001",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064f90fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations / augmentations\n",
    "img_transform_source = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "])\n",
    "\n",
    "img_transform_target = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "dataset_source = datasets.MNIST(\n",
    "    root='dataset',\n",
    "    train=True,\n",
    "    transform=img_transform_source,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Load MNIST-M dataset\n",
    "train_list = os.path.join(target_image_root, 'mnist_m_train_labels.txt')\n",
    "dataset_target = GetLoader(\n",
    "    data_root=os.path.join(target_image_root, 'mnist_m_train'),\n",
    "    data_list=train_list,\n",
    "    transform=img_transform_target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16e77f",
   "metadata": {},
   "source": [
    "# Direct Domain Adaptation (DDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a153b",
   "metadata": {},
   "source": [
    "## Average auto-correlation\n",
    "For entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4726ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load global acorr\n"
     ]
    }
   ],
   "source": [
    "def get_global_acorr_for_loader(loader):\n",
    "    \"\"\" The average auto-correlation of all images in the dataset \"\"\"\n",
    "    global_acorr = np.zeros((3, 28, 15), dtype=np.complex128)\n",
    "    prog_bar = tqdm.tqdm(loader)\n",
    "    for data, _ in prog_bar:\n",
    "        data_f = np.fft.rfft2(data, s=data.shape[-2:], axes=(-2, -1))\n",
    "        # Auto-correlation is multiplication with the conjucate of self\n",
    "        # in frequency domain\n",
    "        global_acorr += data_f * np.conjugate(data_f)\n",
    "    global_acorr /= len(loader)\n",
    "    print(global_acorr.shape)\n",
    "    return np.fft.fftshift(global_acorr)\n",
    "\n",
    "def route_to(fname):\n",
    "    \"\"\" Shortcut for routing to the save folder \"\"\"\n",
    "    return os.path.join(model_root, fname)\n",
    "\n",
    "# Compute global acorr if not in the folder, load otherwise\n",
    "if not 'gacorr_dst_tr.npy' in os.listdir(model_root):\n",
    "    print('Save global acorr')\n",
    "    gacorr_dst_tr = get_global_acorr_for_loader(dataset_target)\n",
    "    gacorr_src_tr = get_global_acorr_for_loader(dataset_source)\n",
    "    np.save(route_to('gacorr_dst_tr.npy'), gacorr_dst_tr)\n",
    "    np.save(route_to('gacorr_src_tr.npy'), gacorr_src_tr)\n",
    "else:\n",
    "    print('Load global acorr')\n",
    "    gacorr_dst_tr = np.load(route_to('gacorr_dst_tr.npy'))\n",
    "    gacorr_src_tr = np.load(route_to('gacorr_src_tr.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a39ef9",
   "metadata": {},
   "source": [
    "## Average cross-correlation\n",
    "Pick a random pixel(-s) from each image in the dataset and average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff54a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the crop size for xcorr = 1\n"
     ]
    }
   ],
   "source": [
    "# Window size\n",
    "crop_size = 1\n",
    "print(f'Use the crop size for xcorr = {crop_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763faf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_ref(x, n=1, edge=5):\n",
    "    \"\"\"Crop a window from the image\n",
    "    Args:\n",
    "        x(np.ndarray): image [c, h, w]\n",
    "        n(int): window size\n",
    "        edge(int): margin to avoid from edges of the image\n",
    "    \"\"\"\n",
    "    if n % 2 == 0: n+=1;\n",
    "    k = int((n - 1) / 2)\n",
    "    nz, nx = x.shape[-2:]\n",
    "    dim1 = np.random.randint(0+k+edge, nz-k-edge)\n",
    "    dim2 = np.random.randint(0+k, nx-k)\n",
    "    out = x[..., dim1-k:dim1+k+1, dim2-k:dim2+k+1]\n",
    "    return out\n",
    "\n",
    "# crop_ref(np.ones((2, 100, 100)), n=5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e82838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load global xcorr\n"
     ]
    }
   ],
   "source": [
    "def get_global_xcorr_for_loader(loader, crop_size=1):\n",
    "    # Init the placeholder for average\n",
    "    rand_pixel = np.zeros((3, crop_size, crop_size))\n",
    "    \n",
    "    # Loop over all images in the dataset\n",
    "    prog_bar = tqdm.tqdm(loader)\n",
    "    for data, _ in prog_bar:\n",
    "        rand_pixel += np.mean(crop_ref(data, crop_size).numpy(), axis=0)\n",
    "    rand_pixel /= len(loader)\n",
    "    \n",
    "    # Place the mean pixel into center of an empty image\n",
    "    c, h ,w = data.shape\n",
    "    mid_h, mid_w = int(h // 2), int(w // 2)\n",
    "    embed = np.zeros_like(data)\n",
    "    embed[..., mid_h:mid_h+1, mid_w:mid_w+1] = rand_pixel\n",
    "    global_xcorr = np.fft.rfft2(embed, s=data.shape[-2:], axes=(-2, -1))\n",
    "    return np.fft.fftshift(global_xcorr)\n",
    "\n",
    "\n",
    "# Compute global xcorr if not in the folder, load otherwise\n",
    "if not 'gxcorr_dst_tr.npy' in os.listdir(model_root):\n",
    "# if True:\n",
    "    print('Save global xcorr')\n",
    "    gxcorr_dst_tr = get_global_xcorr_for_loader(dataset_target, crop_size)\n",
    "    gxcorr_src_tr = get_global_xcorr_for_loader(dataset_source, crop_size)\n",
    "    np.save(route_to('gxcorr_dst_tr.npy'), gxcorr_dst_tr)\n",
    "    np.save(route_to('gxcorr_src_tr.npy'), gxcorr_src_tr)\n",
    "else:\n",
    "    print('Load global xcorr')\n",
    "    gxcorr_dst_tr = np.load(route_to('gxcorr_dst_tr.npy'))\n",
    "    gxcorr_src_tr = np.load(route_to('gxcorr_src_tr.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af19464",
   "metadata": {},
   "source": [
    "## Train Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71df37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_channels(x):\n",
    "    \"\"\"Reverse polarity of random channels\"\"\"\n",
    "    flip_matrix = np.random.choice([-1, 1], 3)[..., np.newaxis, np.newaxis] \n",
    "    return (x * flip_matrix).astype(np.float32)\n",
    "\n",
    "def shuffle_channels(x):\n",
    "    \"\"\"Change order of channels\"\"\"\n",
    "    return np.random.permutation(x)\n",
    "\n",
    "def normalize_channels(x):\n",
    "    \"\"\"Map data to [-1,1] range. The scaling after conv(xcorr, acorr) is not \n",
    "    suitable for image processing so this function fixes it\"\"\"\n",
    "    cmin = np.min(x, axis=(-2,-1))[..., np.newaxis, np.newaxis]\n",
    "    x -= cmin\n",
    "    cmax = np.max(np.abs(x), axis=(-2,-1))[..., np.newaxis, np.newaxis]\n",
    "    x /= cmax\n",
    "    x *= 2\n",
    "    x -= 1\n",
    "    return x.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23c225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDALoaderTrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, loader1, avg_acorr2, p=0.5, crop_size=1):\n",
    "        super().__init__()\n",
    "        self.loader1 = loader1\n",
    "        self.avg_acorr2 = avg_acorr2\n",
    "        self.p = p\n",
    "        self.crop_size = crop_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.loader1)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        # Get main data (data, label)\n",
    "        data, label = self.loader1.__getitem__(item)\n",
    "        data_fft = fftshift(rfft2(data, s=data.shape[-2:], axes=(-2, -1)))\n",
    "        \n",
    "        # Get random pixel from another data from the same dataset\n",
    "        random_index = np.random.randint(0, len(self.loader1))\n",
    "        another_data, _ = self.loader1.__getitem__(random_index)\n",
    "        rand_pixel = crop_ref(another_data, self.crop_size)\n",
    "        \n",
    "        # Convert to Fourier domain\n",
    "        c, h ,w = another_data.shape\n",
    "        mid_h, mid_w = int(h // 2), int(w // 2)\n",
    "        embed = np.zeros_like(another_data)\n",
    "        embed[:, mid_h:mid_h+1, mid_w:mid_w+1] = rand_pixel\n",
    "        pixel_fft = np.fft.rfft2(embed, s=another_data.shape[-2:], axes=(-2, -1))\n",
    "        pixel_fft = np.fft.fftshift(pixel_fft)\n",
    "            \n",
    "        # Cross-correlate the data sample with the random pixel from the same dataset\n",
    "        xcorr = data_fft * np.conjugate(pixel_fft)\n",
    "\n",
    "        # Convolve the ruslt with the auto-correlation of another dataset\n",
    "        conv = xcorr * self.avg_acorr2\n",
    "\n",
    "        # Reverse Fourier domain and map channels to [-1, 1] range\n",
    "        data_da = fftshift(irfft2(ifftshift(conv), axes=(-2, -1)))\n",
    "        data_da = normalize_channels(data_da)\n",
    "        \n",
    "        # Apply data augmentations\n",
    "        if np.random.rand() < self.p:\n",
    "            data_da = flip_channels(data_da)\n",
    "        if np.random.rand() < self.p:\n",
    "            data_da = shuffle_channels(data_da)\n",
    "            \n",
    "        # Return a pair of data / label\n",
    "        return data_da.astype(np.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31e4a991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (3, 28, 28)\t Label: 5\n"
     ]
    }
   ],
   "source": [
    "dataset_source = DDALoaderTrain(dataset_source, gacorr_dst_tr)\n",
    "dataset_target = DDALoaderTrain(dataset_target, gacorr_src_tr)\n",
    "\n",
    "dummy_data, dummy_label = dataset_source.__getitem__(0)\n",
    "print('Image shape: {}\\t Label: {}'.format(dummy_data.shape, dummy_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79bc126",
   "metadata": {},
   "source": [
    "# Test Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8cf6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDALoaderTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, loader1, avg_acorr2, avg_xcorr1):\n",
    "        super().__init__()\n",
    "        self.loader1 = loader1\n",
    "        self.avg_acorr2 = avg_acorr2\n",
    "        self.avg_xcorr1 = avg_xcorr1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.loader1)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        data, label = self.loader1.__getitem__(item)\n",
    "        data_fft = fftshift(rfft2(data, s=data.shape[-2:], axes=(-2, -1)))\n",
    "        \n",
    "        xcorr = data_fft * np.conjugate(self.avg_xcorr1)\n",
    "        conv = xcorr * self.avg_acorr2\n",
    "\n",
    "        data_da = fftshift(irfft2(ifftshift(conv), axes=(-2, -1)))\n",
    "        data_da = normalize_channels(data_da)\n",
    "        return data_da.astype(np.float32), label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3357bd",
   "metadata": {},
   "source": [
    "Re-define the test function so it accounts for the average cross-correlation and auto-correlation from source and target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f64cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset_name, model_root, crop_size=1):\n",
    "    image_root = os.path.join('dataset', dataset_name)  \n",
    "    if dataset_name == 'mnist_m':\n",
    "        test_list = os.path.join(image_root, 'mnist_m_test_labels.txt')\n",
    "        dataset = GetLoader(\n",
    "            data_root=os.path.join(image_root, 'mnist_m_test'),\n",
    "            data_list=test_list,\n",
    "            transform=img_transform_target\n",
    "        )\n",
    "        \n",
    "        if not 'gxcorr_dst_te.npy' in os.listdir(model_root):\n",
    "            print('Save global acorr and xcorr')\n",
    "            # acorr\n",
    "            gacorr_src_te = get_global_acorr_for_loader(dataset)\n",
    "            np.save(route_to('gacorr_src_te.npy'), gacorr_src_te)\n",
    "            # xcorr\n",
    "            gxcorr_dst_te = get_global_xcorr_for_loader(dataset, crop_size)\n",
    "            np.save(route_to('gxcorr_dst_te.npy'), gxcorr_dst_te)\n",
    "        else:\n",
    "            gacorr_src_te = np.load(route_to('gacorr_src_te.npy'))\n",
    "            gxcorr_dst_te = np.load(route_to('gxcorr_dst_te.npy'))\n",
    "            \n",
    "        # Init loader for MNIST-M\n",
    "        dataset = DDALoaderTest(dataset, gacorr_src_te, gxcorr_dst_te)\n",
    "    else:\n",
    "        dataset = datasets.MNIST(\n",
    "            root='dataset',\n",
    "            train=False,\n",
    "            transform=img_transform_source,\n",
    "        )\n",
    "        if not 'gxcorr_src_te.npy' in os.listdir(model_root):\n",
    "            print('Save global acorr and xcorr')\n",
    "            # acorr\n",
    "            gacorr_dst_te = get_global_acorr_for_loader(dataset)\n",
    "            np.save(route_to('gacorr_dst_te.npy'), gacorr_dst_te)\n",
    "            # xcorr\n",
    "            gxcorr_src_te = get_global_xcorr_for_loader(dataset, crop_size)\n",
    "            np.save(route_to('gxcorr_src_te.npy'), gxcorr_src_te)\n",
    "        else:\n",
    "            gacorr_dst_te = np.load(route_to('gacorr_dst_te.npy'))\n",
    "            gxcorr_src_te = np.load(route_to('gxcorr_src_te.npy'))\n",
    "            \n",
    "        # Init loader for MNIST\n",
    "        dataset = DDALoaderTest(dataset, gacorr_dst_te, gxcorr_src_te)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8\n",
    "    )\n",
    "\n",
    "    \"\"\" test \"\"\"\n",
    "    my_net = torch.load(os.path.join(model_root, 'mnist_mnistm_model_epoch_current.pth'))\n",
    "    my_net = my_net.eval()\n",
    "\n",
    "    if cuda:\n",
    "        my_net = my_net.cuda()\n",
    "\n",
    "    len_dataloader = len(dataloader)\n",
    "    data_target_iter = iter(dataloader)\n",
    "\n",
    "    i = 0\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    while i < len_dataloader:\n",
    "        # test model using target data\n",
    "        data_target = data_target_iter.next()\n",
    "        t_img, t_label = data_target\n",
    "\n",
    "        _batch_size = len(t_label)\n",
    "\n",
    "        if cuda:\n",
    "            t_img = t_img.cuda()\n",
    "            t_label = t_label.cuda()\n",
    "\n",
    "        class_output, _ = my_net(input_data=t_img, alpha=0)\n",
    "        pred = class_output.data.max(1, keepdim=True)[1]\n",
    "        n_correct += pred.eq(t_label.data.view_as(pred)).cpu().sum()\n",
    "        n_total += _batch_size\n",
    "        i += 1\n",
    "\n",
    "    accu = n_correct.data.numpy() * 1.0 / n_total\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57efef4f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60c6f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_source = torch.utils.data.DataLoader(\n",
    "    dataset=dataset_source,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8)\n",
    "\n",
    "dataloader_target = torch.utils.data.DataLoader(\n",
    "    dataset=dataset_target,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "537ff640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.feature = nn.Sequential()\n",
    "        self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))\n",
    "        self.feature.add_module('f_bn1', nn.BatchNorm2d(64))\n",
    "        self.feature.add_module('f_pool1', nn.MaxPool2d(2))\n",
    "        self.feature.add_module('f_relu1', nn.ReLU(True))\n",
    "        self.feature.add_module('f_conv2', nn.Conv2d(64, 50, kernel_size=5))\n",
    "        self.feature.add_module('f_bn2', nn.BatchNorm2d(50))\n",
    "        self.feature.add_module('f_drop1', nn.Dropout2d())\n",
    "        self.feature.add_module('f_pool2', nn.MaxPool2d(2))\n",
    "        self.feature.add_module('f_relu2', nn.ReLU(True))\n",
    "\n",
    "        self.class_classifier = nn.Sequential()\n",
    "        self.class_classifier.add_module('c_fc1', nn.Linear(50 * 4 * 4, 100))\n",
    "        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))\n",
    "        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n",
    "        self.class_classifier.add_module('c_drop1', nn.Dropout())\n",
    "        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n",
    "        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n",
    "        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n",
    "        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10))\n",
    "        self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        input_data = input_data.expand(input_data.data.shape[0], 3, 28, 28)\n",
    "        feature = self.feature(input_data)\n",
    "        feature = feature.view(-1, 50 * 4 * 4)\n",
    "\n",
    "        class_output = self.class_classifier(feature)\n",
    "\n",
    "        return class_output, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb1ce3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "my_net = CNNModel()\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = optim.Adam(my_net.parameters(), lr=lr)\n",
    "\n",
    "loss_class = torch.nn.NLLLoss()\n",
    "\n",
    "if cuda:\n",
    "    my_net = my_net.cuda()\n",
    "    loss_class = loss_class.cuda()\n",
    "\n",
    "for p in my_net.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf9f79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record losses for each epoch (used in compare.ipynb)\n",
    "losses = {'test': {'acc_bw': [], 'acc_color': []}}\n",
    "name_losses = 'losses.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1978205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses from previous run found!\n",
      "Load dict from ./out_ex3_cnn_da/losses.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not name_losses in os.listdir(model_root):\n",
    "# if True:\n",
    "    # training\n",
    "    best_accu_t = 0.0\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        len_dataloader = min(len(dataloader_source), len(dataloader_target))\n",
    "        data_source_iter = iter(dataloader_source)\n",
    "        data_target_iter = iter(dataloader_target)\n",
    "\n",
    "        for i in range(len_dataloader):\n",
    "\n",
    "            p = float(i + epoch * len_dataloader) / n_epoch / len_dataloader\n",
    "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "            # training model using source data\n",
    "            data_source = data_source_iter.next()\n",
    "            s_img, s_label = data_source\n",
    "\n",
    "            my_net.zero_grad()\n",
    "            batch_size = len(s_label)\n",
    "\n",
    "            if cuda:\n",
    "                s_img = s_img.cuda()\n",
    "                s_label = s_label.cuda()\n",
    "\n",
    "            class_output, _ = my_net(input_data=s_img, alpha=alpha)\n",
    "            err_s_label = loss_class(class_output, s_label)\n",
    "\n",
    "            err = err_s_label\n",
    "            err.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sys.stdout.write('\\r epoch: %d, [iter: %d / all %d], err_s_label: %f' \\\n",
    "                  % (epoch, i + 1, len_dataloader, err_s_label.data.cpu().numpy()))\n",
    "            sys.stdout.flush()\n",
    "            torch.save(my_net, '{0}/mnist_mnistm_model_epoch_current.pth'.format(model_root))\n",
    "\n",
    "        print('\\n')\n",
    "        accu_s = test(source_dataset_name, model_root)\n",
    "        print('Accuracy of the %s dataset: %f' % ('mnist', accu_s))\n",
    "        accu_t = test(target_dataset_name, model_root)\n",
    "        print('Accuracy of the %s dataset: %f\\n' % ('mnist_m', accu_t))\n",
    "\n",
    "        losses['test']['acc_bw'].append(accu_s)\n",
    "        losses['test']['acc_color'].append(accu_t)\n",
    "\n",
    "        if accu_t > best_accu_t:\n",
    "            best_accu_s = accu_s\n",
    "            best_accu_t = accu_t\n",
    "            torch.save(my_net, '{0}/mnist_mnistm_model_epoch_best.pth'.format(model_root))\n",
    "\n",
    "    print('============ Summary ============= \\n')\n",
    "    print('Accuracy of the %s dataset: %f' % ('mnist', best_accu_s))\n",
    "    print('Accuracy of the %s dataset: %f' % ('mnist_m', best_accu_t))\n",
    "    print('Corresponding model was save in ' + model_root + '/mnist_mnistm_model_epoch_best.pth')\n",
    "    sd.save_dict(os.path.join(model_root, 'losses.pkl'), losses)\n",
    "else:\n",
    "    path_losses = os.path.join(model_root, name_losses)\n",
    "    print(f'Losses from previous run found!')\n",
    "    losses = sd.load_dict(path_losses)\n",
    "    sd.plot_curves(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01148fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Summary ============= \n",
      "\n",
      "Accuracy of the mnist dataset: 0.990200\n",
      "Accuracy of the mnist_m dataset: 0.734141\n",
      "Corresponding model was saved into ./out_ex3_cnn_da/mnist_mnistm_model_epoch_best.pth\n"
     ]
    }
   ],
   "source": [
    "print('============ Summary ============= \\n')\n",
    "print('Accuracy of the %s dataset: %f' % ('mnist', max(losses['test']['acc_bw'])))\n",
    "print('Accuracy of the %s dataset: %f' % ('mnist_m', max(losses['test']['acc_color'])))\n",
    "print('Corresponding model was saved into ' + model_root + '/mnist_mnistm_model_epoch_best.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
